{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from ast import literal_eval\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statannotations.Annotator import Annotator\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# === Setup ===\n",
    "curr_wd = os.path.abspath(os.getcwd())\n",
    "print(f\"Current working directory: {curr_wd}\")\n",
    "\n",
    "# === Utility Functions ===\n",
    "def return_chop_seq(seq, list_of_slices):\n",
    "    \"\"\"Return a new string composed of specific slices from the input sequence.\"\"\"\n",
    "    return ''.join(seq[start:end] for start, end in list_of_slices)\n",
    "\n",
    "def combine_dicts(dicts_list):\n",
    "    \"\"\"Merge list of dictionaries by appending values of same keys.\"\"\"\n",
    "    combined_dict = {}\n",
    "    for dictionary in dicts_list:\n",
    "        for key, value in dictionary.items():\n",
    "            combined_dict.setdefault(key, []).append(value)\n",
    "    return combined_dict\n",
    "\n",
    "def count_consecutive_stretches_of_1(lst):\n",
    "    \"\"\"Find stretches of consecutive 1's in a list.\"\"\"\n",
    "    start_stop_list = []\n",
    "    current_stretch = False\n",
    "    for index, num in enumerate(lst):\n",
    "        if num == 1:\n",
    "            if not current_stretch:\n",
    "                start = index\n",
    "                current_stretch = True\n",
    "        elif current_stretch:\n",
    "            stop = index\n",
    "            start_stop_list.append((start, stop, \"IDR\"))\n",
    "            current_stretch = False\n",
    "    if current_stretch:\n",
    "        start_stop_list.append((start, len(lst), \"IDR\"))\n",
    "    return start_stop_list\n",
    "\n",
    "def is_either_between(low_range, high_range, a, b):\n",
    "    \"\"\"Check if either a or b is within a given range.\"\"\"\n",
    "    return (low_range <= a <= high_range) or (low_range <= b <= high_range)\n",
    "\n",
    "def assign_group(protein, group1_name, group1_string, group2_name, group2_string):\n",
    "    \"\"\"Assign protein to one of two groups.\"\"\"\n",
    "    if protein in group1_name:\n",
    "        return group1_string\n",
    "    elif protein in group2_name:\n",
    "        return group2_string\n",
    "    else:\n",
    "        return 'Not in any group'\n",
    "\n",
    "def assign_group_4(protein, group1_name, group1_string, group2_name, group2_string, group3_name, group3_string, group4_name, group4_string):\n",
    "    \"\"\"Assign protein to one of four groups.\"\"\"\n",
    "    if protein in group1_name:\n",
    "        return group1_string\n",
    "    elif protein in group2_name:\n",
    "        return group2_string\n",
    "    elif protein in group3_name:\n",
    "        return group3_string\n",
    "    elif protein in group4_name:\n",
    "        return group4_string\n",
    "    else:\n",
    "        return 'Not in any group'\n",
    "\n",
    "def create_boxplot_with_dots(data, hue_order, scatter=True, **kwargs):\n",
    "    \"\"\"Create boxplot with optional scatter overlay and statistical annotations.\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = sns.boxplot(data=data, y='data', x='Group', hue='metric', width=0.8,\n",
    "                     order=[\"pos\", \"neg\"], hue_order=hue_order,\n",
    "                     showfliers=False, ax=plt.gca(), zorder=5, **kwargs)\n",
    "    \n",
    "    if scatter:\n",
    "        sns.stripplot(data=data, y='data', x='Group', hue='metric', color='black',\n",
    "                      order=[\"pos\", \"neg\"], hue_order=hue_order,\n",
    "                      size=5, jitter=0.1, alpha=0.3, dodge=True, zorder=10)\n",
    "\n",
    "    pairs = [[('pos', metric), ('neg', metric)] for metric in hue_order]\n",
    "    annotator = Annotator(plt.gca(), pairs, data=data, y='data', x='Group', hue='metric')\n",
    "    annotator.configure(test='Mann-Whitney', text_format='star', loc='outside',\n",
    "                        hide_non_significant=False, verbose=0)\n",
    "    annotator.apply_and_annotate()\n",
    "\n",
    "    plt.xlabel(\"GAR positive group vs negative group\")\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.legend().remove()\n",
    "    ax.grid(axis='y', zorder=0)\n",
    "    ax.set_axisbelow(True)\n",
    "    plt.close()\n",
    "\n",
    "    return fig, annotator\n",
    "\n",
    "def mult_count(seq, list_of_letters):\n",
    "    \"\"\"Count total occurrences of specified letters in a sequence.\"\"\"\n",
    "    return sum(seq.count(letter) for letter in list_of_letters)\n",
    "\n",
    "def pval_to_asterisk(value):\n",
    "    \"\"\"Convert p-value to significance string.\"\"\"\n",
    "    if value > 0.05 or np.isnan(value):\n",
    "        return 'ns'\n",
    "    elif value > 0.01:\n",
    "        return '*'\n",
    "    elif value > 0.001:\n",
    "        return '**'\n",
    "    elif value > 0.0001:\n",
    "        return '***'\n",
    "    else:\n",
    "        return \"****\"\n",
    "\n",
    "# === Constants ===\n",
    "aa_type_to_aa_dict = {\n",
    "    \"aliphatic\": \"GAVLMI\",\n",
    "    \"aromatic\": \"FYW\",\n",
    "    \"pos_charged\": \"KRH\",\n",
    "    \"neg_charged\": \"DE\",\n",
    "    \"uncharged\": \"STCNPQ\",\n",
    "    \"aliphatic_noG\": \"AVLMI\",\n",
    "    \"pos_charged_noR\": \"KH\"\n",
    "}\n",
    "aa_types_all = list(aa_type_to_aa_dict.keys())\n",
    "aa_all = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "# === Load Data ===\n",
    "motif_info_set_df = pd.read_parquet(\n",
    "    os.path.join(curr_wd, 'data/processed/GAR_motif_Wang_set_human_cleaned_annot_filtered.parquet')\n",
    ")\n",
    "annotated_IDR_df = pd.read_parquet(\n",
    "    os.path.join(curr_wd, 'data/processed/annotation_datasets/all_IDR_human.parquet')\n",
    ")\n",
    "annotated_domain_df = pd.read_parquet(\n",
    "    os.path.join(curr_wd, 'data/processed/annotation_datasets/all_domains_human.parquet')\n",
    ")\n",
    "annotated_PTMs_df = pd.read_parquet(\n",
    "    os.path.join(curr_wd, 'data/processed/annotation_datasets/all_PTMs_human.parquet')\n",
    ")\n",
    "\n",
    "# Prepare PTM to AA mapping\n",
    "df2 = annotated_PTMs_df.dropna(subset=['AA', 'ptm']).drop_duplicates(subset=['AA', 'ptm'])\n",
    "ptm_to_aa_dict = {ptm: df2[df2[\"ptm\"] == ptm][\"AA\"].iloc[0] for ptm in df2[\"ptm\"]}\n",
    "print(ptm_to_aa_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = \"v3\"\n",
    "\n",
    "# Define named sets and their associated file names\n",
    "set_definitions = {\n",
    "    \"GAR_full\": [\"GAR_subset_full\"],\n",
    "    \"GAR_LLPS_pos\": [\n",
    "        \"4_LLPS_positive_set_and_GAR_subset\",\n",
    "        \"5_LLPS_positive_set_and_NA_positive_set_and_GAR_subset\"\n",
    "    ],\n",
    "    \"GAR_LLPS_pos_NA_neg\": [\"4_LLPS_positive_set_and_GAR_subset\"],\n",
    "    \"GAR_LLPS_neg\": [\n",
    "        \"6_NA_positive_set_and_GAR_subset\",\n",
    "        \"7_GAR_subset_only\"\n",
    "    ],\n",
    "    \"GAR_LLPS_neg_NA_pos\": [\"6_NA_positive_set_and_GAR_subset\"],\n",
    "    \"GAR_NA_pos\": [\n",
    "        \"5_LLPS_positive_set_and_NA_positive_set_and_GAR_subset\",\n",
    "        \"6_NA_positive_set_and_GAR_subset\"\n",
    "    ],\n",
    "    \"GAR_NA_neg\": [\n",
    "        \"4_LLPS_positive_set_and_GAR_subset\",\n",
    "        \"7_GAR_subset_only\"\n",
    "    ],\n",
    "    \"GAR_pos\": [\"5_LLPS_positive_set_and_NA_positive_set_and_GAR_subset\"],\n",
    "    \"GAR_neg\": [\"7_GAR_subset_only\"],\n",
    "}\n",
    "\n",
    "# Initialize dictionaries and containers\n",
    "set_dict = {}\n",
    "set_list = []\n",
    "proteins_sets_dict = {}\n",
    "\n",
    "# Load each set of proteins from its respective files\n",
    "for set_name, file_names in set_definitions.items():\n",
    "    proteins = []\n",
    "    for fname in file_names:\n",
    "        file_path = f\"{curr_wd}/data/processed/final_set_lists/{fname}.txt\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            proteins.extend(line.strip() for line in f)\n",
    "    set_dict[set_name] = proteins\n",
    "    set_list.append(proteins)\n",
    "\n",
    "# Load full proteome\n",
    "full_proteome_path = f\"{curr_wd}/data/processed/list_of_human_proteins.csv\"\n",
    "with open(full_proteome_path, \"r\") as f:\n",
    "    full_proteome = [line.strip() for line in f]\n",
    "set_dict[\"full_proteome\"] = full_proteome\n",
    "set_list.append(full_proteome)\n",
    "\n",
    "# Store sets in versioned dictionary\n",
    "set_names = list(set_dict.keys())\n",
    "proteins_sets_dict[ver] = set_dict\n",
    "\n",
    "my_pos_group = \"GAR_pos\"\n",
    "my_neg_group = \"GAR_neg\"\n",
    "\n",
    "GAR_LLPS_pos_NA_neg_group = \"GAR_LLPS_pos_NA_neg\"\n",
    "GAR_LLPS_neg_NA_pos_group = \"GAR_LLPS_neg_NA_pos\"\n",
    "\n",
    "\n",
    "motif_info_set_df['Group'] = motif_info_set_df['UniqueID'].apply(\n",
    "                                                                    assign_group_4,\n",
    "                                                                    args= (\n",
    "                                                                            set_list[set_names.index(my_pos_group)], \"pos\",\n",
    "                                                                            set_list[set_names.index(my_neg_group)], \"neg\",\n",
    "                                                                            set_list[set_names.index(GAR_LLPS_pos_NA_neg_group)], \"LLPS_pos_NA_neg\",\n",
    "                                                                            set_list[set_names.index(GAR_LLPS_neg_NA_pos_group)], \"LLPS_neg_NA_pos\",\n",
    "                                                                            \n",
    "                                                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dict_with_aa_metrics = {}\n",
    "\n",
    "for i, curr_protein in enumerate(proteins_sets_dict['v3']['full_proteome']):\n",
    "    # Check for missing motif info\n",
    "    curr_motif_df = motif_info_set_df[motif_info_set_df[\"UniqueID\"] == curr_protein]\n",
    "    if curr_motif_df.empty:\n",
    "        continue\n",
    "\n",
    "    full_sequence = curr_motif_df['full_seq'].iloc[0]\n",
    "    prot_length = len(full_sequence)\n",
    "\n",
    "    # Check for missing or invalid IDR info\n",
    "    curr_idr_df = annotated_IDR_df[annotated_IDR_df[\"protein_name\"] == curr_protein]\n",
    "    if curr_idr_df.empty:\n",
    "        continue\n",
    "\n",
    "    IDR_info = curr_idr_df[\"prediction-disorder-mobidb_lite\"].iloc[0]\n",
    "    if isinstance(IDR_info, list) and IDR_info.count(-1) > 0:\n",
    "        continue\n",
    "\n",
    "    print(i, \" out of \", len(proteins_sets_dict['v3']['full_proteome']))\n",
    "    print(curr_protein)\n",
    "\n",
    "    protein_dict_with_aa_metrics[curr_protein] = {}\n",
    "    IDR_info = IDR_info.tolist() if isinstance(IDR_info, np.ndarray) else IDR_info  # Make sure it's a list\n",
    "\n",
    "    # Count IDR types\n",
    "    count_0 = IDR_info.count(0)\n",
    "    count_1 = IDR_info.count(1)\n",
    "    motif_1s = 0\n",
    "    motif_IDR_bounds = []\n",
    "\n",
    "    # === Identify motif-associated IDRs ===\n",
    "    for _, m in curr_motif_df[[\"start\", \"end\"]].iterrows():\n",
    "        if m['start'] >= len(IDR_info):\n",
    "            break\n",
    "\n",
    "        # Skip overlapping motifs\n",
    "        if any(is_either_between(lr, hr, m['start'], m['end']) for lr, hr in motif_IDR_bounds):\n",
    "            continue\n",
    "\n",
    "        if IDR_info[m['start']] == 1:\n",
    "            # Expand left\n",
    "            low_range = m['start']\n",
    "            while low_range > 0 and IDR_info[low_range - 1] == 1:\n",
    "                low_range -= 1\n",
    "                motif_1s += 1\n",
    "            # Expand right\n",
    "            high_range = m['start']\n",
    "            while high_range < len(IDR_info) and IDR_info[high_range] == 1:\n",
    "                high_range += 1\n",
    "                motif_1s += 1\n",
    "\n",
    "        elif IDR_info[m['end'] - 1] == 1:\n",
    "            low_range = m['end'] - 1\n",
    "            while low_range > 0 and IDR_info[low_range] == 1:\n",
    "                low_range -= 1\n",
    "                motif_1s += 1\n",
    "            high_range = m['end']\n",
    "            while high_range < len(IDR_info) and IDR_info[high_range] == 1:\n",
    "                high_range += 1\n",
    "                motif_1s += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        motif_IDR_bounds.append((low_range, high_range))\n",
    "\n",
    "    # === Determine IDR stats ===\n",
    "    len_non_IDR = count_0\n",
    "    len_all_IDRs = count_1\n",
    "    len_motif_IDR = motif_1s\n",
    "    len_other_IDR = count_1 - motif_1s\n",
    "\n",
    "    all_IDR_bounds = count_consecutive_stretches_of_1(IDR_info)\n",
    "\n",
    "    protein_dict_with_aa_metrics[curr_protein].update({\n",
    "        'num_of_IDR_regions': len(all_IDR_bounds),\n",
    "        'num_of_IDR_regions_w_motif': len(motif_IDR_bounds),\n",
    "        'num_of_IDR_regions_wo_motif': len(all_IDR_bounds) - len(motif_IDR_bounds),\n",
    "        'motif_IDR_range': motif_IDR_bounds,\n",
    "        'all_IDR_ranges': all_IDR_bounds,\n",
    "    })\n",
    "\n",
    "    # === Identify sequence regions ===\n",
    "    non_IDR_bounds = []\n",
    "    other_IDR_bounds = []\n",
    "    last_end = 0\n",
    "\n",
    "    for bnd in all_IDR_bounds:\n",
    "        if bnd[0] > last_end:\n",
    "            non_IDR_bounds.append((last_end, bnd[0]))\n",
    "        last_end = bnd[1]\n",
    "\n",
    "        if not any(is_either_between(bnd[0], bnd[1], mb[0], mb[1]) for mb in motif_IDR_bounds):\n",
    "            other_IDR_bounds.append((bnd[0], bnd[1]))\n",
    "\n",
    "    if last_end < prot_length:\n",
    "        non_IDR_bounds.append((last_end, prot_length))\n",
    "\n",
    "    seq_outside_IDR = return_chop_seq(full_sequence, non_IDR_bounds)\n",
    "    seq_other_IDR = return_chop_seq(full_sequence, other_IDR_bounds)\n",
    "\n",
    "    # === Per-motif and per-AA analysis ===\n",
    "    for i2, row2 in curr_motif_df.iterrows():\n",
    "        protein_dict_with_aa_metrics[curr_protein][i2] = {}\n",
    "        motif_length = len(row2['motif'])\n",
    "        start = row2[\"start\"] - 1\n",
    "        end = row2[\"end\"]\n",
    "\n",
    "        # Find matching motif_IDR_bounds\n",
    "        for mIb in motif_IDR_bounds:\n",
    "            if is_either_between(mIb[0], mIb[1], row2[\"start\"], row2[\"end\"]):\n",
    "                curr_motif_IDR_bounds = mIb\n",
    "                break\n",
    "\n",
    "        for aa in aa_all + aa_types_all:\n",
    "            aa_str = aa_type_to_aa_dict[aa] if aa in aa_types_all else aa\n",
    "\n",
    "            aa_metrics = {\n",
    "                \"counts_overall\": mult_count(full_sequence, aa_str),\n",
    "                \"density_overall\": mult_count(full_sequence, aa_str) / prot_length,\n",
    "                \"counts_outside_IDR\": np.nan,\n",
    "                \"density_outside_IDR\": np.nan,\n",
    "                \"counts_in_other_IDR\": np.nan,\n",
    "                \"density_in_other_IDRs\": np.nan,\n",
    "                \"counts_in_motif_IDR\": np.nan,\n",
    "                \"density_in_motif_IDR\": np.nan,\n",
    "                \"counts_in_motif\": mult_count(row2['motif'], aa_str),\n",
    "                \"density_in_motif\": mult_count(row2['motif'], aa_str) / motif_length,\n",
    "            }\n",
    "\n",
    "            # +/- window metrics\n",
    "            for step in range(1, 11):\n",
    "                plus_seq = full_sequence[end + (step - 1) - 1 : end + step]\n",
    "                minus_seq = full_sequence[start - step - 1 : start - (step - 1)]\n",
    "                aa_metrics[f\"counts_in_motif+{step}\"] = mult_count(plus_seq, aa_str) if len(plus_seq) else np.nan\n",
    "                aa_metrics[f\"counts_in_motif-{step}\"] = mult_count(minus_seq, aa_str) if len(minus_seq) else np.nan\n",
    "\n",
    "            # Region-specific counts\n",
    "            if seq_outside_IDR:\n",
    "                aa_metrics[\"counts_outside_IDR\"] = mult_count(seq_outside_IDR, aa_str)\n",
    "                aa_metrics[\"density_outside_IDR\"] = aa_metrics[\"counts_outside_IDR\"] / len_non_IDR\n",
    "\n",
    "            if seq_other_IDR:\n",
    "                aa_metrics[\"counts_in_other_IDR\"] = mult_count(seq_other_IDR, aa_str)\n",
    "                aa_metrics[\"density_in_other_IDRs\"] = aa_metrics[\"counts_in_other_IDR\"] / len_other_IDR\n",
    "\n",
    "            motif_seq = full_sequence[curr_motif_IDR_bounds[0]:curr_motif_IDR_bounds[1]]\n",
    "            aa_metrics[\"counts_in_motif_IDR\"] = mult_count(motif_seq, aa_str)\n",
    "            aa_metrics[\"density_in_motif_IDR\"] = aa_metrics[\"counts_in_motif_IDR\"] / (curr_motif_IDR_bounds[1] - curr_motif_IDR_bounds[0])\n",
    "\n",
    "            protein_dict_with_aa_metrics[curr_protein][i2][aa] = aa_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Flatten nested dictionary into lists\n",
    "records = []\n",
    "for prot, motifs in protein_dict_with_aa_metrics.items():\n",
    "    for motif, aas in motifs.items():\n",
    "        if isinstance(aas, dict):\n",
    "            for aa, metrics in aas.items():\n",
    "                for metric, data in metrics.items():\n",
    "                    records.append((prot, motif, aa, metric, data))\n",
    "\n",
    "# Create DataFrame\n",
    "protein_df = pd.DataFrame(records, columns=['prot', 'motif', 'aa', 'metric', 'data'])\n",
    "print(len(protein_df))\n",
    "\n",
    "# Drop NaN data entries\n",
    "protein_df = protein_df.dropna(subset=['data'])\n",
    "print(len(protein_df))\n",
    "\n",
    "# Distinguish between individual amino acids and types\n",
    "aa_all = [x for x in protein_df['aa'].unique() if len(x) == 1]\n",
    "aa_types_all = [x for x in protein_df['aa'].unique() if len(x) > 1]\n",
    "print(aa_all)\n",
    "print(aa_types_all)\n",
    "\n",
    "# Collect all metric types\n",
    "metrics_all = list(protein_df['metric'].unique())\n",
    "print(metrics_all)\n",
    "\n",
    "# Define group assignment\n",
    "def assign_groups_advanced(protein, g1, g1_label, g2, g2_label, g3, g3_label, g4, g4_label):\n",
    "    if protein in g1:\n",
    "        return g1_label\n",
    "    elif protein in g2:\n",
    "        return g2_label\n",
    "    elif protein in g3:\n",
    "        return g3_label\n",
    "    elif protein in g4:\n",
    "        return g4_label\n",
    "    return 'Not in any group'\n",
    "\n",
    "# Apply group labels (uses predefined variables: set_list, set_names, my_pos_group, my_neg_group)\n",
    "protein_df['Group'] = protein_df['prot'].apply(\n",
    "    assign_group, args=(\n",
    "        set_list[set_names.index(my_pos_group)], \"pos\",\n",
    "        set_list[set_names.index(my_neg_group)], \"neg\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Infer data types\n",
    "protein_df = protein_df.infer_objects()\n",
    "\n",
    "# Compute statistics\n",
    "statistics_dict = {}\n",
    "for metric in metrics_all:\n",
    "    statistics_dict[metric] = {}\n",
    "\n",
    "    for aa in protein_df['aa'].unique():\n",
    "        aa_mask = protein_df['aa'] == aa\n",
    "        metric_mask = protein_df['metric'] == metric\n",
    "        pos_data = protein_df[aa_mask & metric_mask & (protein_df['Group'] == \"pos\")]['data'].tolist()\n",
    "        neg_data = protein_df[aa_mask & metric_mask & (protein_df['Group'] == \"neg\")]['data'].tolist()\n",
    "\n",
    "        mwu_stat, mwu_p = mannwhitneyu(pos_data, neg_data)\n",
    "        mean_pos = np.mean(pos_data) + 1 / len(neg_data)\n",
    "        mean_neg = np.mean(neg_data) + 1 / len(pos_data)\n",
    "\n",
    "        statistics_dict[metric][aa] = [mean_pos, mean_neg, mwu_stat, mwu_p]\n",
    "\n",
    "    # Multiple testing correction (BH)\n",
    "    p_vals = [v[3] for v in statistics_dict[metric].values()]\n",
    "    bh_corrected = multipletests(p_vals, method='fdr_bh')[1]\n",
    "    for aa, corrected_p in zip(statistics_dict[metric], bh_corrected):\n",
    "        statistics_dict[metric][aa].append(corrected_p)\n",
    "\n",
    "# Output\n",
    "print(list(statistics_dict.keys()))\n",
    "print(statistics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{curr_wd}/data/results/proteins_with_aa_metrics_df.pkl\"\n",
    "with open(output_path, \"wb\") as fp:\n",
    "    pickle.dump(protein_df, fp)\n",
    "    print(\"DF saved successfully to file.\")\n",
    "\n",
    "output_path = f\"{curr_wd}/data/results/proteins_with_aa_metrics_statistics_dict.pkl\"\n",
    "with open(output_path, \"wb\") as fp:\n",
    "    pickle.dump(statistics_dict, fp)\n",
    "    print(\"Dict saved successfully to file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "\n",
    "with open(f\"{curr_wd}/data/results/proteins_with_aa_metrics_statistics_dict.pkl\", 'rb') as fp:\n",
    "    statistics_dict = pickle.load(fp)\n",
    "\n",
    "print(\"missing are: I, H, M, T, Q ,S\")\n",
    "\n",
    "def create_heatmap(\n",
    "    data,\n",
    "    metrics_to_portray,\n",
    "    aa_to_portray=list(),\n",
    "    change_label_dict_y=dict(),\n",
    "    change_label_dict_x=dict(),\n",
    "    vmax=None,\n",
    "    cut_ns=True,\n",
    "    sort_map=True,\n",
    "    cell_size=10,\n",
    "    output_data=False\n",
    "):\n",
    "    # Determine amino acids to portray\n",
    "    if len(aa_to_portray) == 0:\n",
    "        all_aa = list(next(iter(data.values())).keys())\n",
    "    else:\n",
    "        all_aa = aa_to_portray\n",
    "\n",
    "    num_rows = len(all_aa)\n",
    "    num_cols = len(metrics_to_portray)\n",
    "\n",
    "    custom_xticks = metrics_to_portray\n",
    "    custom_yticks = all_aa\n",
    "\n",
    "    # Collect fold changes, significance labels, and raw p-values\n",
    "    fold_changes, sign_labels, num_labels = [], [], []\n",
    "    for aa in custom_yticks:\n",
    "        row_fc, row_sign, row_num = [], [], []\n",
    "        for met in custom_xticks:\n",
    "            if data[met][aa][1] == 0 and data[met][aa][0] == 0:\n",
    "                fold_change = np.log2(1)\n",
    "            else:\n",
    "                fold_change = np.log2(data[met][aa][0] / data[met][aa][1])\n",
    "            pval_fdr = data[met][aa][4]\n",
    "            row_fc.append(fold_change)\n",
    "            row_sign.append(pval_to_asterisk(pval_fdr))\n",
    "            row_num.append(pval_fdr)\n",
    "        fold_changes.append(row_fc)\n",
    "        sign_labels.append(row_sign)\n",
    "        num_labels.append(row_num)\n",
    "\n",
    "    # Replace infinities with vmax if given\n",
    "    fold_changes = [[vmax if math.isinf(v) else v for v in row] for row in fold_changes]\n",
    "\n",
    "    # Remove rows where all significance labels are \"ns\"\n",
    "    if cut_ns:\n",
    "        filtered_fc, filtered_sign, filtered_labels, filtered_num = [], [], [], []\n",
    "        for i, row in enumerate(sign_labels):\n",
    "            if not all(lbl == \"ns\" for lbl in row):\n",
    "                filtered_sign.append(sign_labels[i])\n",
    "                filtered_fc.append(fold_changes[i])\n",
    "                filtered_labels.append(custom_yticks[i])\n",
    "                filtered_num.append(num_labels[i])\n",
    "        fold_changes, sign_labels, custom_yticks, num_labels = filtered_fc, filtered_sign, filtered_labels, filtered_num\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    fold_changes = np.array(fold_changes)\n",
    "    sign_labels = np.array(sign_labels)\n",
    "\n",
    "    num_cols = fold_changes.shape[1]\n",
    "    num_rows = fold_changes.shape[0]\n",
    "\n",
    "    # Optionally sort rows by total fold change\n",
    "    if sort_map:\n",
    "        indices = sorted(range(len(fold_changes)), key=lambda i: sum(fold_changes[i]), reverse=True)\n",
    "        fold_changes = fold_changes[indices]\n",
    "        sign_labels = sign_labels[indices]\n",
    "        custom_yticks = [custom_yticks[i] for i in indices]\n",
    "        num_labels = [num_labels[i] for i in indices]\n",
    "\n",
    "    # Define color scale\n",
    "    if vmax is None:\n",
    "        vmax = np.nanmax(fold_changes)\n",
    "        vmin = np.nanmin(fold_changes)\n",
    "        print(vmax)\n",
    "        print(vmin)\n",
    "\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        'custom_colormap',\n",
    "        [(0, '#FF4040'), (0.5, 'white'), (1, '#8DB600')],\n",
    "        N=256\n",
    "    )\n",
    "\n",
    "    # Plot setup\n",
    "    fig = plt.figure(figsize=(num_cols * 1.305, num_rows * 0.495))\n",
    "    plt.imshow(\n",
    "        fold_changes,\n",
    "        cmap=cmap,\n",
    "        vmin=-vmax,\n",
    "        vmax=vmax,\n",
    "        extent=[0, num_cols, num_rows, 0],\n",
    "        aspect=0.66\n",
    "    )\n",
    "\n",
    "    # Cell annotation\n",
    "    x_positions = np.arange(num_cols) + 0.5\n",
    "    y_positions = np.arange(num_rows) + 0.5\n",
    "    for i, y in enumerate(y_positions):\n",
    "        for j, x in enumerate(x_positions):\n",
    "            plt.text(x, y, sign_labels[i][j], ha='center', va='center', fontsize=10, color='black')\n",
    "\n",
    "    # Tick labels\n",
    "    plt.xticks(x_positions, custom_xticks, rotation=0, ha='right')\n",
    "    plt.yticks(y_positions, custom_yticks, va='center', size=11)\n",
    "    plt.gca().tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Apply y-axis label replacements\n",
    "    y_labels = [item.get_text() for item in plt.gca().get_yticklabels()]\n",
    "    for k, v in change_label_dict_y.items():\n",
    "        try:\n",
    "            y_labels[y_labels.index(k)] = v\n",
    "        except ValueError:\n",
    "            print(f\"Label {k} could not be found!\")\n",
    "    plt.gca().set_yticklabels(y_labels)\n",
    "\n",
    "    # Apply x-axis label replacements\n",
    "    x_labels = [item.get_text() for item in plt.gca().get_xticklabels()]\n",
    "    plt.gca().set_xticklabels([change_label_dict_x.get(lbl, lbl) for lbl in x_labels])\n",
    "\n",
    "    # Assemble output data\n",
    "    data_dict = {}\n",
    "    for i, row in enumerate(fold_changes):\n",
    "        data_dict[custom_yticks[i]] = {}\n",
    "        for j, val in enumerate(row):\n",
    "            data_dict[custom_yticks[i]][custom_xticks[j]] = (val, num_labels[i][j])\n",
    "    data_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "    # Axis labels and colorbar\n",
    "    plt.setp(plt.gca().get_xticklabels(), rotation=0, ha=\"center\", rotation_mode=\"anchor\")\n",
    "    plt.colorbar(\n",
    "        fraction=0.25,\n",
    "        location='right',\n",
    "        pad=0.001 * len(y_labels)\n",
    "    ).set_label(\"log2(fold change of means)\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"amino acids\", size=12, labelpad=20)\n",
    "\n",
    "    return (fig, data_df) if output_data else fig\n",
    "\n",
    "# Optional label mapping for x-axis\n",
    "xlabel_dict = {\n",
    "    \"density_overall\": \"full\\nprotein\",\n",
    "    \"density_outside_IDR\": \"ordered\\nregions\",\n",
    "    \"density_in_other_IDRs\": \"oIDRs\",\n",
    "    \"density_in_motif_IDR\": \"mIDRs\",\n",
    "    \"density_in_motif\": \"RG-\\nmotif\"\n",
    "}\n",
    "\n",
    "this_fig, this_data = create_heatmap(statistics_dict, [\"density_outside_IDR\", \"density_in_other_IDRs\",\"density_in_motif_IDR\", \"density_in_motif\"],\n",
    "                        ['K', 'Y', 'N', 'D', 'E',  'F','R', 'G','S', 'H', \"I\", \"M\", \"Q\", \"T\",  'P', 'V', 'W', 'A', 'C','L'], change_label_dict_x = xlabel_dict, vmax=None, sort_map =True, output_data=True\n",
    "               )\n",
    "\n",
    "os.makedirs(os.path.join(curr_wd, \"data/results/subfigures/\"), exist_ok=True)\n",
    "this_fig.savefig(os.path.join(curr_wd, \"data/results/subfigures/fig4_A.svg\"), transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Chosen colors and labels\n",
    "colors = ['#73A5EB', '#C487ED', '#E6E05C', '#E69454']\n",
    "labels = ['oIDRs', 'ordered regions',  'mIDRs', 'RG-motif']\n",
    "\n",
    "# Create custom legend handles\n",
    "handles = [Patch(color=color, label=label) for color, label in zip(colors, labels)]\n",
    "\n",
    "# Create a dummy figure for the legend\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('off')\n",
    "\n",
    "legend = ax.legend(handles=handles, ncol=4, loc='center', frameon=True)\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(curr_wd, \"data/results/subfigures/\"), exist_ok=True)\n",
    "this_fig.savefig(os.path.join(curr_wd, \"data/results/subfigures/fig4_B.svg\"), transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from typing import List\n",
    "\n",
    "# === Constants ===\n",
    "SAVE_PATH = \"data/results/subfigures/suppl_fig_S5.svg\"\n",
    "\n",
    "# === Load cleaned AA-metric data ===\n",
    "with open(f\"{curr_wd}/data/results/proteins_with_aa_metrics_df.pkl\", 'rb') as fp:\n",
    "    protein_df = pickle.load(fp)\n",
    "\n",
    "# === Split positive/negative groups ===\n",
    "cut_data_pos = protein_df[protein_df['Group'] == \"pos\"]\n",
    "cut_data_neg = protein_df[protein_df['Group'] == \"neg\"]\n",
    "\n",
    "# === Heatmap plotting function ===\n",
    "def plot_corr_heatmap(data: pd.DataFrame, aa_list: List[str], group_label: str):\n",
    "    \"\"\"\n",
    "    Plots a correlation heatmap of metric values across amino acids for a protein group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for only the relevant AAs\n",
    "    filtered = data[data['aa'].isin(aa_list)]\n",
    "    filtered = filtered[filtered['metric'] == 'counts_in_motif'].copy()\n",
    "\n",
    "    # Combine AA and metric into a single column\n",
    "    filtered['merged_metric'] = filtered['aa'] + \"_\" + filtered['metric']\n",
    "\n",
    "    # Pivot to get protein x merged_metric matrix\n",
    "    reshaped = filtered.pivot_table(index='prot', columns='merged_metric', values='data')\n",
    "\n",
    "    # Sort columns by AA and relative motif position if present\n",
    "    def sort_key(col):\n",
    "        prefix, suffix = col.split('_', 1)\n",
    "        offset = suffix.split('motif')[-1]\n",
    "        return (prefix, int(offset) if offset else 0)\n",
    "\n",
    "    reshaped = reshaped[sorted(reshaped.columns, key=sort_key)]\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = reshaped.corr(method='pearson')\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    cmap = sns.light_palette(\"#3b4c1f\", as_cmap=True)\n",
    "\n",
    "    sns.heatmap(corr_matrix, annot=False, cbar=True, cmap=cmap)\n",
    "\n",
    "    # Optional: adapt ticks if desired\n",
    "\n",
    "    plt.xticks(np.arange(0.5, len(aa_all), 1.0), ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'], rotation=0)\n",
    "    plt.yticks(np.arange(0.5, len(aa_all), 1.0), ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'], rotation=0)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"AA + metric position\")\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "    # Save figure\n",
    "    os.makedirs(os.path.join(curr_wd, \"data/results/subfigures/\"), exist_ok=True)\n",
    "    save_file = os.path.join(curr_wd, SAVE_PATH.replace(\"suppl_fig_S5\", f\"suppl_fig_S5_{group_label}\"))\n",
    "    plt.savefig(save_file, transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Plot for both groups ===\n",
    "plot_corr_heatmap(cut_data_pos, aa_all, group_label=\"pos\")\n",
    "plot_corr_heatmap(cut_data_neg, aa_all, group_label=\"neg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Filter data by group\n",
    "cut_data_pos = protein_df[protein_df['Group'] == \"pos\"]\n",
    "cut_data_neg = protein_df[protein_df['Group'] == \"neg\"]\n",
    "\n",
    "def plot_corr_heat_map_detailed(cut_data, chosen_aa, group_label, outdir):\n",
    "    # Filter by selected amino acids and metric\n",
    "    cut_data = cut_data[cut_data['aa'].isin(chosen_aa)].copy()\n",
    "    cut_data['merged_metric'] = cut_data['aa'] + \"_\" + cut_data['metric']\n",
    "\n",
    "    # Keep only selected metrics\n",
    "    selected_metrics = ['counts_in_motif']\n",
    "    cut_data = cut_data[cut_data['metric'].isin(selected_metrics)]\n",
    "\n",
    "    # Pivot table: proteins x (AA_position)\n",
    "    matrix = cut_data.pivot_table(index='prot', columns='merged_metric', values='data')\n",
    "\n",
    "    # Sort columns by AA and numeric suffix in the metric name\n",
    "    def sort_key(col):\n",
    "        aa, rest = col.split('_', 1)\n",
    "        suffix = rest.split('motif')[1]\n",
    "        offset = int(suffix) if suffix else 0\n",
    "        return aa, offset\n",
    "\n",
    "    matrix = matrix[sorted(matrix.columns, key=sort_key)]\n",
    "\n",
    "    # Compute correlation\n",
    "    corr_matrix = matrix.corr(method='pearson')\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    cmap = sns.light_palette(\"#3b4c1f\", as_cmap=True)\n",
    "    heatmap = sns.heatmap(\n",
    "        corr_matrix, annot=True, cbar=True, cmap=cmap,\n",
    "        annot_kws={\"fontsize\": 12}, square=True\n",
    "    )\n",
    "\n",
    "    # Adjust tick labels\n",
    "    tick_labels = chosen_aa\n",
    "    plt.xticks(np.arange(0.5, len(tick_labels), 1.0), tick_labels, rotation=0, fontsize=12)\n",
    "    plt.yticks(np.arange(0.5, len(tick_labels), 1.0), tick_labels, rotation=0, fontsize=12)\n",
    "    plt.xlabel(\"amino acids\", fontsize=14)\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "    # Save figure\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    filename = f\"fig5_E\" if group_label == \"pos\" else \"fig5_F\"\n",
    "    plt.savefig(os.path.join(outdir, filename + \".svg\"), transparent=True)\n",
    "    # plt.close()\n",
    "\n",
    "# Run for both groups\n",
    "chosen_aa = list(\"FGRWY\")\n",
    "output_dir = os.path.join(curr_wd, \"data/results/subfigures/\")\n",
    "plot_corr_heat_map_detailed(cut_data_pos, chosen_aa, \"pos\", output_dir)\n",
    "plot_corr_heat_map_detailed(cut_data_neg, chosen_aa, \"neg\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "import pandas as pd\n",
    "\n",
    "# === Parameters ===\n",
    "letter1 = 'Y'\n",
    "letter2 = 'F'\n",
    "threshold = 5\n",
    "window = 30\n",
    "group_used = [\"pos\"]\n",
    "\n",
    "# Output path\n",
    "output_path = os.path.join(curr_wd, \"data/results/subfigures/\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# === Filter motif dataframe ===\n",
    "used_motif_df = motif_info_set_df[motif_info_set_df[\"Group\"].isin(group_used)]\n",
    "\n",
    "# === Count motifs with at least 'threshold' of letter1/letter2 in the ±window region ===\n",
    "def motif_has_letter(row, letter):\n",
    "    seq = str(row[\"full_seq\"][row[\"start\"] - 1 - window : row[\"end\"] + window])\n",
    "    return seq.count(letter) >= threshold\n",
    "\n",
    "mask_A = used_motif_df.apply(lambda row: motif_has_letter(row, letter1), axis=1)\n",
    "mask_B = used_motif_df.apply(lambda row: motif_has_letter(row, letter2), axis=1)\n",
    "mask_AB = mask_A & mask_B\n",
    "\n",
    "count_A = mask_A.sum()\n",
    "count_B = mask_B.sum()\n",
    "count_AB = mask_AB.sum()\n",
    "\n",
    "print(f\"Motifs with ≥{threshold} '{letter1}': {count_A}\")\n",
    "print(f\"Motifs with ≥{threshold} '{letter2}': {count_B}\")\n",
    "print(f\"Motifs with both ≥{threshold} '{letter1}' and '{letter2}': {count_AB}\")\n",
    "\n",
    "# === Venn diagram ===\n",
    "plt.figure(figsize=(4, 4))\n",
    "venn = venn2(\n",
    "    subsets=(count_A - count_AB, count_B - count_AB, count_AB),\n",
    "    set_labels=(\"\", \"\"),\n",
    "    set_colors=(\"mediumvioletred\", \"lightseagreen\"),\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Aesthetic tweaks\n",
    "for i in [0, 1, 2]:\n",
    "    patch = venn.patches[i]\n",
    "    if patch:\n",
    "        patch.set_edgecolor(\"grey\")\n",
    "        patch.set_linewidth(2)\n",
    "\n",
    "for label in venn.subset_labels:\n",
    "    if label:\n",
    "        label.set_fontsize(14)\n",
    "\n",
    "# Save\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, \"fig5_A.svg\"), transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gseapy as gp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mygene\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def convert_uniprot_to_gene_symbols(uniprot_ids):\n",
    "    \"\"\"Converts a list of UniProt IDs to Entrez Gene Symbols using MyGene.info.\"\"\"\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    query_result = mg.querymany(uniprot_ids, scopes='uniprot', fields='symbol', species='human')\n",
    "    \n",
    "    symbol_counts = {}\n",
    "    gene_symbols = []\n",
    "    \n",
    "    duplicates_checked = {}\n",
    "    \n",
    "    for entry in query_result:\n",
    "        if \"symbol\" in entry:\n",
    "            symbol = entry[\"symbol\"]\n",
    "            if symbol not in duplicates_checked:\n",
    "                duplicates_checked[symbol] = []\n",
    "            duplicates_checked[symbol].append(entry)\n",
    "            \n",
    "    for symbol, entries in duplicates_checked.items():\n",
    "        unique_symbols = set(e[\"symbol\"] for e in entries if \"symbol\" in e)\n",
    "        if len(unique_symbols) == 1:\n",
    "            gene_symbols.append(symbol)\n",
    "        else:\n",
    "            print(f\"Potential ambiguous mapping for {symbol}: {unique_symbols}\")\n",
    "    \n",
    "    return gene_symbols\n",
    "\n",
    "def run_enrichment(protein_list, library=\"GO_Biological_Process_2021\", max_retries=3):\n",
    "    \"\"\"Runs Enrichr enrichment analysis for a given protein list and GO library.\"\"\"\n",
    "    gene_symbols = convert_uniprot_to_gene_symbols(protein_list)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            enr = gp.enrichr(gene_list=gene_symbols, gene_sets=library, organism='human', outdir=None)\n",
    "            if enr.results is None or enr.results.empty:\n",
    "                print(f\"No significant enrichment found for {library}.\")\n",
    "                return None\n",
    "            return enr.results.sort_values(by=\"Adjusted P-value\").head(10)  # Top 10 terms\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    \n",
    "    print(\"Enrichr request failed after multiple attempts.\")\n",
    "    return None\n",
    "\n",
    "def plot_enrichment(enrichment_results, title):\n",
    "    \"\"\"Plots enrichment results as a horizontal bar plot.\"\"\"\n",
    "    if enrichment_results is None:\n",
    "        print(f\"No significant enrichment found for {title}.\")\n",
    "        return\n",
    "    \n",
    "    enrichment_results[\"-log10(Adjusted P-value)\"] = -enrichment_results[\"Adjusted P-value\"].apply(lambda x: np.log10(x))\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(\n",
    "        data=enrichment_results, \n",
    "        y=\"Term\", \n",
    "        x=\"-log10(Adjusted P-value)\", \n",
    "        palette=\"viridis\"\n",
    "    )\n",
    "    plt.xlabel(\"-log10(Adjusted P-value)\")\n",
    "    plt.ylabel(\"GO Term\")\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # Highest enrichment at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "proteins_A = used_motif_df.loc[mask_A, \"UniqueID\"].tolist()\n",
    "# print(proteins_A)\n",
    "\n",
    "proteins_B = used_motif_df.loc[mask_B, \"UniqueID\"].tolist()\n",
    "# print(proteins_B)\n",
    "\n",
    "proteins_AB = list(set(proteins_A) & set(proteins_B))\n",
    "\n",
    "protein_list1 = list(set([item for item in proteins_A if item not in proteins_AB])) # [\"P04637\", \"P00533\", \"P38398\", \"P42345\", \"P06400\"]  # UniProt IDs\n",
    "protein_list2 = list(set([item for item in proteins_B if item not in proteins_AB]))# [\"P01106\", \"P24941\", \"P11802\", \"P24385\", \"Q15796\"]  # UniProt IDs\n",
    "\n",
    "go_terms = [\"GO_Biological_Process_2021\", \"GO_Molecular_Function_2021\", \"GO_Cellular_Component_2021\"]\n",
    "results_1 = []\n",
    "results_2 = []\n",
    "for go in go_terms:\n",
    "    results_1.append(run_enrichment(protein_list1, go))\n",
    "    results_2.append(run_enrichment(protein_list2, go))\n",
    "\n",
    "\n",
    "# Save to file\n",
    "with open(curr_wd + '/data/results/' + 'protein_for_Y.pkl', 'wb') as f:\n",
    "    pickle.dump(results_1, f)\n",
    "\n",
    "with open(curr_wd + '/data/results/' + 'protein_for_F.pkl', 'wb') as f:\n",
    "    pickle.dump(results_2, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Load from file\n",
    "with open(curr_wd + '/data/results/' + 'protein_for_Y.pkl', 'rb') as f:\n",
    "    results_1 = pickle.load(f)\n",
    "\n",
    "# Load from file\n",
    "with open(curr_wd + '/data/results/' + 'protein_for_F.pkl', 'rb') as f:\n",
    "    results_2 = pickle.load(f)\n",
    "\n",
    "\n",
    "def shorten_label(label, max_length=50):\n",
    "    \"\"\"Shortens labels while preserving the GO term in parentheses.\"\"\"\n",
    "    if len(label) <= max_length:\n",
    "        return label\n",
    "    if \"(\" in label and \")\" in label:\n",
    "        term = label[label.rfind(\"(\"):]  # Extract GO term in parentheses\n",
    "        base = label[:max_length - len(term) - 1].rstrip()  # Truncate before GO term\n",
    "        return f\"{base}… {term}\" if base else term  # Avoid empty base\n",
    "    return label[:max_length - 1] + \"…\"  # Fallback truncation\n",
    "\n",
    "def plot_enrichment(enrichment_results1, enrichment_results2, title):\n",
    "    \"\"\"Plots enrichment results as a horizontal bar plot with labels on top of the bars.\"\"\"\n",
    "\n",
    "    # Filter for significant terms (Adjusted P-value < 0.01)\n",
    "    enrichment_results1 = enrichment_results1[enrichment_results1[\"Adjusted P-value\"] < 0.01].copy()\n",
    "    enrichment_results2 = enrichment_results2[enrichment_results2[\"Adjusted P-value\"] < 0.01].copy()\n",
    "\n",
    "    # If no terms remain after filtering, return\n",
    "    if enrichment_results1.empty and enrichment_results2.empty:\n",
    "        print(f\"No significant enrichment found for {title}.\")\n",
    "        return\n",
    "\n",
    "    enrichment_results1[\"-log10(Adjusted P-value)\"] = -np.log10(enrichment_results1[\"Adjusted P-value\"])\n",
    "    enrichment_results2[\"-log10(Adjusted P-value)\"] = -np.log10(enrichment_results2[\"Adjusted P-value\"])\n",
    "\n",
    "    num_terms1 = len(enrichment_results1)\n",
    "    num_terms2 = len(enrichment_results2)\n",
    "    \n",
    "    # Height ratios to maintain equal bar widths\n",
    "    height_ratios = [num_terms1, num_terms2] if num_terms1 and num_terms2 else [1, 1]\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        2, 1, sharex=True, figsize=(5.2, 1.5+ 0.25*(num_terms1 + num_terms2)), gridspec_kw={\"height_ratios\": height_ratios}\n",
    "    )\n",
    "\n",
    "    # Define custom colormap\n",
    "    custom_colors_Y = [\"#feb4e3\", \"#ca549e\"]  # Light to dark pink shades\n",
    "    custom_colors_F = [\"#c6fbf9\", \"#71c6c2\"]\n",
    "    custom_cmap_Y = LinearSegmentedColormap.from_list(\"custom_rdpu\", custom_colors_Y, N=256)\n",
    "    custom_cmap_F = LinearSegmentedColormap.from_list(\"custom_rdpu\", custom_colors_F, N=256)\n",
    "    \n",
    "    # Identify shared terms\n",
    "    terms1 = set(enrichment_results1[\"Term\"])\n",
    "    terms2 = set(enrichment_results2[\"Term\"])\n",
    "    shared_terms = terms1 & terms2  # Terms appearing in both group\n",
    "    for ax, enrichment_results, group_name in zip(\n",
    "        axs, \n",
    "        [enrichment_results1, enrichment_results2], \n",
    "        [\"tyrosine protein group\", \"phenylalanine protein group\"]\n",
    "    ):\n",
    "        enrichment_results = enrichment_results.copy()\n",
    "        enrichment_results[\"Short Term\"] = enrichment_results[\"Term\"].apply(shorten_label)\n",
    "\n",
    "        # Normalize values for color mapping\n",
    "        values = -enrichment_results[\"-log10(Adjusted P-value)\"]\n",
    "        norm_values = (values - values.min()) / (values.max() - values.min()) if len(values) > 1 else [0.5] * len(values)\n",
    "        if group_name == \"tyrosine protein group\":\n",
    "            colors = [custom_cmap_Y(val) for val in norm_values]\n",
    "        else:\n",
    "            colors = [custom_cmap_F(val) for val in norm_values]\n",
    "\n",
    "        sns.barplot(\n",
    "            data=enrichment_results, \n",
    "            y=\"Short Term\", \n",
    "            x=\"-log10(Adjusted P-value)\", \n",
    "            palette=colors, ax=ax\n",
    "        )\n",
    "\n",
    "        # Remove y-axis labels and ticks\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_yticklabels([])\n",
    "        ax.tick_params(axis='y', left=False)\n",
    "        ax.set_xlabel(\"\")\n",
    "        \n",
    "        # Ensure x-axis tick labels are visible\n",
    "        # ax.tick_params(axis='x', labelbottom=True)\n",
    "\n",
    "        # Set subplot title\n",
    "        # ax.set_title(group_name, fontsize=12, fontweight=\"bold\", rotation=90)\n",
    "        \n",
    "        # Add text labels on top of bars\n",
    "        for bar, term, label in zip(ax.patches, enrichment_results[\"Term\"], enrichment_results[\"Short Term\"]):\n",
    "            fontweight = \"normal\" if term in shared_terms else \"bold\"\n",
    "            ax.text(\n",
    "                0.1,  # Position at the end of the bar\n",
    "                bar.get_y() + bar.get_height() / 2,  # Centered vertically\n",
    "                label,\n",
    "                ha=\"left\", va=\"center\", fontsize=10, fontweight=fontweight, color=\"black\"\n",
    "            )\n",
    "\n",
    "    axs[-1].set_xlabel(\"-log10(Adjusted P-value)\")\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")  # Main title\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(curr_wd, \"data/results/subfigures/\"), exist_ok=True)\n",
    "    if title == \"GO Biological Process\":\n",
    "        plt.savefig(os.path.join(curr_wd, \"data/results/subfigures/fig5_B.svg\"), transparent=True)\n",
    "    elif title == \"GO Molecular Function\":\n",
    "        plt.savefig(os.path.join(curr_wd, \"data/results/subfigures/fig5_C.svg\"), transparent=True)\n",
    "    elif title == \"GO Cellular Component\":\n",
    "        plt.savefig(os.path.join(curr_wd, \"data/results/subfigures/fig5_D.svg\"), transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for i, go in enumerate([\"GO Biological Process\", \"GO Molecular Function\", \"GO Cellular Component\"]):\n",
    "    plot_enrichment(results_1[i], results_2[i], f\"{go}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "\n",
    "def find_all_occurrences(s, char, add=0):\n",
    "    \"\"\"Find all positions of `char` in `s`, offset by `add`.\"\"\"\n",
    "    return [i + add for i, c in enumerate(s) if c == char]\n",
    "\n",
    "def signed_distances_to_region_rel(positions, region, length=1):\n",
    "    \"\"\"Compute signed normalized distances of positions to a region.\"\"\"\n",
    "    start, end = region\n",
    "    distances = []\n",
    "    for p in positions:\n",
    "        if p < start:\n",
    "            distances.append((p - start) / length)  # Negative: left of region\n",
    "        elif p > end:\n",
    "            distances.append((p - end) / length)    # Positive: right of region\n",
    "    return distances\n",
    "\n",
    "# Define the amino acids to scan for\n",
    "chars_to_find = aa_all  # Previously: [\"Y\", \"N\", \"F\", \"M\", \"D\", \"K\", \"A\"]\n",
    "\n",
    "# Split data into groups\n",
    "pos_motif_df = motif_info_set_df[motif_info_set_df[\"Group\"] == \"pos\"]\n",
    "neg_motif_df = motif_info_set_df[motif_info_set_df[\"Group\"] == \"neg\"]\n",
    "\n",
    "pos_new_dict = {}\n",
    "neg_new_dict = {}\n",
    "\n",
    "# Compute distances for each amino acid of interest\n",
    "for let in chars_to_find:\n",
    "    temp_pos_new_data = []\n",
    "    for _, r in pos_motif_df.iterrows():\n",
    "        motif_IDR_ranges = protein_dict_with_aa_metrics[r[\"UniqueID\"]][\"motif_IDR_range\"]\n",
    "        corr_rng = next((rng for rng in motif_IDR_ranges if r[\"start\"] > rng[0] and r[\"end\"] < rng[1]), None)\n",
    "        if corr_rng is None:\n",
    "            continue  # Skip if no matching range is found\n",
    "        pre_seq = r[\"full_seq\"][corr_rng[0]:r[\"start\"]]\n",
    "        post_seq = r[\"full_seq\"][r[\"start\"]:corr_rng[1]]\n",
    "        positions = find_all_occurrences(r[\"full_seq\"][corr_rng[0]:corr_rng[1]], let, corr_rng[0])\n",
    "        distances = signed_distances_to_region_rel(positions, (r[\"start\"], r[\"end\"]))\n",
    "        temp_pos_new_data.append(distances)\n",
    "    pos_new_dict[let] = temp_pos_new_data\n",
    "\n",
    "    temp_neg_new_data = []\n",
    "    for _, r in neg_motif_df.iterrows():\n",
    "        motif_IDR_ranges = protein_dict_with_aa_metrics[r[\"UniqueID\"]][\"motif_IDR_range\"]\n",
    "        corr_rng = next((rng for rng in motif_IDR_ranges if r[\"start\"] > rng[0] and r[\"end\"] < rng[1]), None)\n",
    "        if corr_rng is None:\n",
    "            continue\n",
    "        positions = find_all_occurrences(r[\"full_seq\"][corr_rng[0]:corr_rng[1]], let, corr_rng[0])\n",
    "        distances = signed_distances_to_region_rel(positions, (r[\"start\"], r[\"end\"]))\n",
    "        temp_neg_new_data.append(distances)\n",
    "    neg_new_dict[let] = temp_neg_new_data\n",
    "\n",
    "# Collect subsequences around motifs for positive group\n",
    "all_pos_pre_seqs = []\n",
    "all_pos_post_seqs = []\n",
    "all_pos_motifs = []\n",
    "all_pos_full_length = []\n",
    "\n",
    "for _, r in pos_motif_df.iterrows():\n",
    "    motif_IDR_ranges = protein_dict_with_aa_metrics[r[\"UniqueID\"]][\"motif_IDR_range\"]\n",
    "    corr_rng = next((rng for rng in motif_IDR_ranges if r[\"start\"] > rng[0] and r[\"end\"] < rng[1]), None)\n",
    "    if corr_rng is None:\n",
    "        continue\n",
    "    all_pos_pre_seqs.append(r[\"full_seq\"][corr_rng[0]:r[\"start\"] - 1])\n",
    "    all_pos_motifs.append(r[\"motif\"])\n",
    "    all_pos_post_seqs.append(r[\"full_seq\"][r[\"end\"]:corr_rng[1]])\n",
    "    all_pos_full_length.append(r[\"full_seq\"][corr_rng[0]:corr_rng[1]])\n",
    "\n",
    "# Collect subsequences around motifs for negative group\n",
    "all_neg_pre_seqs = []\n",
    "all_neg_post_seqs = []\n",
    "\n",
    "for _, r in neg_motif_df.iterrows():\n",
    "    motif_IDR_ranges = protein_dict_with_aa_metrics[r[\"UniqueID\"]][\"motif_IDR_range\"]\n",
    "    corr_rng = next((rng for rng in motif_IDR_ranges if r[\"start\"] > rng[0] and r[\"end\"] < rng[1]), None)\n",
    "    if corr_rng is None:\n",
    "        continue\n",
    "    all_neg_pre_seqs.append(r[\"full_seq\"][corr_rng[0]:r[\"start\"]])\n",
    "    all_neg_post_seqs.append(r[\"full_seq\"][r[\"end\"] + 1:corr_rng[1]])\n",
    "\n",
    "def extract_sequences_from_fasta(fasta_file):\n",
    "    \"\"\"Extract sequences from FASTA, keyed by UniProt ID.\"\"\"\n",
    "    sequences = {}\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        record_id = record.id.split(\"|\")[1]\n",
    "        sequences[record_id] = str(record.seq)\n",
    "    return sequences\n",
    "\n",
    "def filter_sequences(sequences, masks):\n",
    "    \"\"\"Mask sequences using boolean mask from disorder annotations.\"\"\"\n",
    "    filtered_sequences = {}\n",
    "    for identifier, seq in sequences.items():\n",
    "        if identifier in masks:\n",
    "            mask = masks[identifier]\n",
    "            if len(seq) == len(mask):\n",
    "                filtered_seq = ''.join([res for res, flag in zip(seq, mask) if flag])\n",
    "                filtered_sequences[identifier] = filtered_seq\n",
    "    return filtered_sequences\n",
    "\n",
    "def compute_aa_proportions(sequences):\n",
    "    \"\"\"Compute mean and SEM of amino acid proportions across sequences.\"\"\"\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    proportions = {aa: [] for aa in amino_acids}\n",
    "    \n",
    "    for seq in sequences.values():\n",
    "        seq_len = len(seq)\n",
    "        if seq_len == 0:\n",
    "            continue\n",
    "        counts = Counter(seq)\n",
    "        for aa in amino_acids:\n",
    "            proportions[aa].append(counts.get(aa, 0) / seq_len)\n",
    "    \n",
    "    avg_proportions = {aa: 100 * np.mean(proportions[aa]) for aa in amino_acids}\n",
    "    sem_proportions = {\n",
    "        aa: 100 * (np.std(proportions[aa], ddof=1) / np.sqrt(len(proportions[aa])))\n",
    "        for aa in amino_acids\n",
    "    }\n",
    "    \n",
    "    return avg_proportions, sem_proportions\n",
    "\n",
    "# Load sequences and apply IDR masking\n",
    "sequences = extract_sequences_from_fasta('/mnt/d/phd/scripts/raw_data/proteomes/UP000005640_9606.fasta')\n",
    "masks = annotated_IDR_df.set_index(\"protein_name\")[\"prediction-disorder-mobidb_lite\"].to_dict()\n",
    "filtered = filter_sequences(sequences, masks)\n",
    "\n",
    "# Compute AA proportions in filtered IDRs\n",
    "avg, sem = compute_aa_proportions(filtered)\n",
    "print(\"Average Proportions:\", avg)\n",
    "print(\"Sum of averages:\", sum(avg.values()))\n",
    "print(\"Standard Error:\", sem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "def calculate_aa_percentage(sequence, aa_list):\n",
    "    \"\"\"Compute the percentage of each amino acid in a sequence.\"\"\"\n",
    "    counts = Counter(sequence)\n",
    "    total = len(sequence) if sequence else 1  # Avoid division by zero\n",
    "    return {aa: (counts[aa] / total) * 100 for aa in aa_list}\n",
    "\n",
    "def compute_proportions(sequences, amino_acids, scope, window_size=1):\n",
    "    \"\"\"Compute amino acid proportions and standard deviation at each position.\"\"\"\n",
    "    aa_percentages = {aa: [] for aa in amino_acids}\n",
    "    aa_stddev = {aa: [] for aa in amino_acids}\n",
    "    aa_counts = {aa: [] for aa in amino_acids}\n",
    "\n",
    "    for i in range(min(scope, 1), max(scope, -1) + 1):\n",
    "        if scope > 0:\n",
    "            merged_seqs = [seq[i-1:i+window_size] for seq in sequences if len(seq) > i+2]\n",
    "        else:\n",
    "            merged_seqs = [seq[i - 1 - window_size:i] for seq in sequences if len(seq) > abs(i)]\n",
    "\n",
    "        proportions_list = [calculate_aa_percentage(seq, amino_acids) for seq in merged_seqs]\n",
    "\n",
    "        for aa in amino_acids:\n",
    "            values = [p.get(aa, 0) for p in proportions_list]\n",
    "            aa_percentages[aa].append(np.mean(values))\n",
    "            aa_stddev[aa].append(np.std(values))\n",
    "            aa_counts[aa].append(max(1, len(values)))  # Avoid division by zero\n",
    "\n",
    "    return aa_percentages, aa_stddev, aa_counts\n",
    "\n",
    "def plot_aa_proportion(pre_pos, post_pos, pre_neg, post_neg, amino_acids,\n",
    "                       ref_props, ref_sems, ylim_values=None, window_size=1, scope=100, gap=1):\n",
    "    \"\"\"Plot amino acid proportions for positions flanking motifs.\"\"\"\n",
    "    pre_pos_props, pre_pos_stddev, pre_pos_counts = compute_proportions(pre_pos, amino_acids, -scope, window_size)\n",
    "    post_pos_props, post_pos_stddev, post_pos_counts = compute_proportions(post_pos, amino_acids, scope, window_size)\n",
    "    pre_neg_props, pre_neg_stddev, pre_neg_counts = compute_proportions(pre_neg, amino_acids, -scope, window_size)\n",
    "    post_neg_props, post_neg_stddev, post_neg_counts = compute_proportions(post_neg, amino_acids, scope, window_size)\n",
    "\n",
    "    fig, axes = plt.subplots(len(amino_acids) // 4, 4, figsize=(5.5 * 4, 1 + 2/4 * len(amino_acids)), sharex=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (ax, aa) in enumerate(zip(axes, amino_acids), start=1):\n",
    "        pre_len = len(pre_pos_props[aa])\n",
    "        post_len = len(post_pos_props[aa])\n",
    "\n",
    "        xticks_pre = np.arange(-pre_len, 0)\n",
    "        xticks_post = np.arange(1, post_len + 1) + gap\n",
    "\n",
    "        # Compute SEMs\n",
    "        sem_pre_pos = np.array(pre_pos_stddev[aa]) / np.sqrt(pre_pos_counts[aa])\n",
    "        sem_post_pos = np.array(post_pos_stddev[aa]) / np.sqrt(post_pos_counts[aa])\n",
    "        sem_pre_neg = np.array(pre_neg_stddev[aa]) / np.sqrt(pre_neg_counts[aa])\n",
    "        sem_post_neg = np.array(post_neg_stddev[aa]) / np.sqrt(post_neg_counts[aa])\n",
    "\n",
    "        # Reference line\n",
    "        if aa in ref_props:\n",
    "            ax.axhline(ref_props[aa], color=\"black\", linestyle=\"dashed\", label=f\"Avg {aa} in Disordered Regions\")\n",
    "\n",
    "        # Plot shaded error regions\n",
    "        ax.fill_between(xticks_pre, pre_pos_props[aa] - sem_pre_pos, pre_pos_props[aa] + sem_pre_pos, color='#8DB600', alpha=0.2)\n",
    "        ax.fill_between(xticks_post, post_pos_props[aa] - sem_post_pos, post_pos_props[aa] + sem_post_pos, color='#8DB600', alpha=0.2)\n",
    "        ax.fill_between(xticks_pre, pre_neg_props[aa] - sem_pre_neg, pre_neg_props[aa] + sem_pre_neg, color='#FF4040', alpha=0.2)\n",
    "        ax.fill_between(xticks_post, post_neg_props[aa] - sem_post_neg, post_neg_props[aa] + sem_post_neg, color='#FF4040', alpha=0.2)\n",
    "\n",
    "        # Plot lines\n",
    "        ax.plot(xticks_pre, pre_pos_props[aa], marker=\"o\", markersize=4, linewidth=2, color='#8DB600', label=\"Pre-Motif (Pos)\")\n",
    "        ax.plot(xticks_post, post_pos_props[aa], marker=\"o\", markersize=4, linewidth=2, color='#8DB600')\n",
    "        ax.plot(xticks_pre, pre_neg_props[aa], marker=\"o\", markersize=4, linewidth=2, color='#FF4040', label=\"Pre-Motif (Neg)\")\n",
    "        ax.plot(xticks_post, post_neg_props[aa], marker=\"o\", markersize=4, linewidth=2, color='#FF4040')\n",
    "\n",
    "        ax.set_ylabel(f\"Proportion of {aa} (%)\", size=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Highlight motif region\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        ax.fill_between([-1, 2], y_min - 5, y_max + 10, color=\"gray\", alpha=0.3)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        if ylim_values and aa in ylim_values:\n",
    "            ax.set_ylim(ylim_values[aa])\n",
    "\n",
    "        # Set custom x-ticks\n",
    "        xticks_major = np.concatenate([\n",
    "            range(xticks_pre[-1], xticks_pre[0]-1, -scope//5),\n",
    "            range(xticks_post[0], xticks_post[-1]+1, scope//5)\n",
    "        ])\n",
    "        xticks_minor = np.concatenate([\n",
    "            range(xticks_pre[-1], xticks_pre[0]-1, -1),\n",
    "            range(xticks_post[0], xticks_post[-1]+1)\n",
    "        ])\n",
    "        ax.set_xticks(xticks_major)\n",
    "        ax.set_xticks(xticks_minor, minor=True)\n",
    "        ax.set_xticklabels(xticks_major, size=9)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), size=9)\n",
    "        ax.set_xlim(-scope - 1, scope + gap + 1)\n",
    "\n",
    "        if i > 16:\n",
    "            ax.set_xlabel(\"Amino acid position relative to RG-motif\", size=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(curr_wd, \"data/results/subfigures/\"), exist_ok=True)\n",
    "    plt.savefig(os.path.join(curr_wd, \"data/results/subfigures/suppl_fig_S4.svg\"), transparent=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "selected_aas = aa_all\n",
    "ref_y_props = avg\n",
    "ref_y_sem = sem\n",
    "ylim_manual = {\"A\": (0, 25), \"F\": (0, 5), \"R\": (0, 25), \"G\": (0, 25)}\n",
    "\n",
    "plot_aa_proportion(\n",
    "    all_pos_pre_seqs, all_pos_post_seqs,\n",
    "    all_neg_pre_seqs, all_neg_post_seqs,\n",
    "    selected_aas,\n",
    "    ref_y_props, ref_y_sem,\n",
    "    ylim_values=None,\n",
    "    window_size=4,\n",
    "    scope=40\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
