{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis \n",
    "print(os.path.abspath(os.getcwd()))\n",
    "curr_wd = os.path.abspath(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOADING AND CLEANING THE HUMAN DATASET, AND SAVING A LIST OF ALL HUMAN PROTEINS\n",
    "\n",
    "### Load FASTA file and extract UniProt identifiers, sequences, and descriptions\n",
    "with open(curr_wd + '/data/external/UniProt/UP000005640_9606.fasta') as fasta_file:\n",
    "    identifiers, seqs, desc = [], [], []\n",
    "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):\n",
    "        identifiers.append(seq_record.id)\n",
    "        seqs.append(str(seq_record.seq))\n",
    "        desc.append(seq_record.description)\n",
    "\n",
    "# Construct dataframe from parsed FASTA entries\n",
    "pseqs_df = pd.DataFrame({\n",
    "    'ID': identifiers,\n",
    "    'sequences': seqs,\n",
    "    'description': desc\n",
    "})\n",
    "\n",
    "# Split UniProt identifier into database, unique identifier, and entry name\n",
    "pseqs_df[['db', 'UniqueIdentifier', 'EntryName']] = pseqs_df['ID'].str.split('|', expand=True)\n",
    "pseqs_df.drop(columns='ID', inplace=True)\n",
    "\n",
    "# Extract gene names from description field, if available\n",
    "genenames = []\n",
    "for el in pseqs_df['description']:\n",
    "    if \"GN=\" not in el:\n",
    "        genenames.append(\"None\")\n",
    "    else:\n",
    "        temp = el.split(\" \")\n",
    "        for el2 in reversed(temp):\n",
    "            if el2.startswith(\"GN=\"):\n",
    "                genenames.append(el2[3:])\n",
    "                break\n",
    "pseqs_df['gene_name'] = genenames\n",
    "\n",
    "### Compute amino acid composition, R/G percentages, and sequence length\n",
    "countdict = []\n",
    "rperclist, gperclist, ratiolist = [], [], []\n",
    "\n",
    "for seq in pseqs_df['sequences']:\n",
    "    analysis = ProteinAnalysis(seq)\n",
    "    aa_counts = analysis.count_amino_acids()\n",
    "    aa_perc = analysis.get_amino_acids_percent()\n",
    "\n",
    "    countdict.append(aa_counts)\n",
    "    rperclist.append(100 * aa_perc['R'])\n",
    "    gperclist.append(100 * aa_perc['G'])\n",
    "\n",
    "    if aa_counts['G'] == 0:\n",
    "        ratiolist.append(None)\n",
    "    else:\n",
    "        ratiolist.append(round(aa_counts['R'] / aa_counts['G'], 3))\n",
    "\n",
    "# Add computed features to dataframe\n",
    "pseqs_df['Else'] = countdict  # Full amino acid counts\n",
    "pseqs_df['length'] = pseqs_df['sequences'].apply(len)\n",
    "pseqs_df['R%'] = rperclist\n",
    "pseqs_df['G%'] = gperclist\n",
    "pseqs_df['R%+G%'] = pseqs_df['R%'] + pseqs_df['G%']\n",
    "pseqs_df['RtoG_ratio'] = ratiolist\n",
    "\n",
    "# Remove short sequences (length â‰¤ 10)\n",
    "pseqs_df = pseqs_df[pseqs_df['length'] > 10]\n",
    "\n",
    "### Save list of all unique UniProt identifiers in the human proteome\n",
    "pseqs_df['UniqueIdentifier'].to_csv(\n",
    "    curr_wd + '/data/processed/list_of_human_proteins.csv',\n",
    "    index=False,\n",
    "    header=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD AND CLEAN GAR MOTIF DATASET FOR HUMAN PROTEOME\n",
    "\n",
    "def annotate_RGG_RG_count(setA):\n",
    "    # Count the occurrences of \"RGG\" motif in the 'motif' column and store the counts in a new column 'RGG_count'.\n",
    "    setA['RGG_count'] = setA['motif'].apply(lambda x: x.count(\"RGG\"))\n",
    "    # Count the occurrences of \"RG\" motif (without counting the \"RGG\" occurences in the 'motif' column and store the counts in a new column 'RG_count'.\n",
    "    setA['RG_count'] = setA['motif'].apply(lambda x: x.count(\"RG\") - x.count(\"RGG\"))\n",
    "    # Count the occurrences of true \"RG\" motif in the 'motif' column and store the counts in a new column 'true_RG_count'.\n",
    "    setA['true_RG_count'] = setA['motif'].apply(lambda x: x.count(\"RG\"))\n",
    "    \n",
    "    return setA\n",
    "\n",
    "# Load motif dataset from Wang et al. (GAR motif finder)\n",
    "with open(curr_wd + '/data/external/GAR_motif_finder/UP000005640_9606.f.csv', \"r\") as f:\n",
    "    tempset3 = pd.read_csv(f)\n",
    "\n",
    "# Fill down missing accession numbers\n",
    "for i, e in tempset3.iterrows():\n",
    "    if pd.isnull(e['Accession number']):\n",
    "        tempset3.at[i, 'Accession number'] = tempset3.at[i - 1, 'Accession number']\n",
    "\n",
    "# Extract start and end positions from Position column\n",
    "tempset3[['start', 'end']] = tempset3['Position'].str.split(r'\\.\\.', n=1, expand=True)\n",
    "tempset3['start'] = pd.to_numeric(tempset3['start'])\n",
    "tempset3['end'] = pd.to_numeric(tempset3['end'])\n",
    "\n",
    "# Split Accession number into database, unique UniProt ID, and entry name\n",
    "tempset3[['db', 'UniqueID', 'EntryName']] = tempset3['Accession number'].str.split('|', expand=True)\n",
    "tempset3['EntryName'] = tempset3['EntryName'].str.split(\" \").str[0]\n",
    "\n",
    "# Calculate motif length\n",
    "tempset3['length_motif'] = tempset3['end'] - tempset3['start'] + 1\n",
    "\n",
    "# Parse stringified dictionaries in Else column\n",
    "tempset3['Else'] = tempset3['Else'].apply(lambda x: ast.literal_eval(str(x)))\n",
    "\n",
    "# Compute R-to-G ratio within GAR motif\n",
    "tempset3['r_to_g_ratio'] = tempset3['Gs/Rs in GAR'] ** (-1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "tempset3.rename(columns={\n",
    "    \"Pattern\": \"motif\",\n",
    "    \"G%\": \"g_perc\",\n",
    "    \"R%\": \"r_perc\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Retrieve full sequences from reference human proteome dataframe\n",
    "tempset3['full_seq'] = [\n",
    "    pseqs_df[pseqs_df['UniqueIdentifier'] == uid]['sequences'].tolist()[0]\n",
    "    for uid in tempset3[\"UniqueID\"].tolist()\n",
    "]\n",
    "\n",
    "# Select and reorder relevant columns\n",
    "set3 = tempset3[[\n",
    "    'UniqueID', 'EntryName', 'motif', 'r_to_g_ratio',\n",
    "    'g_perc', 'r_perc', 'start', 'end', 'full_seq', 'Else'\n",
    "]]\n",
    "\n",
    "set3 = annotate_RGG_RG_count(set3)\n",
    "\n",
    "# Save cleaned dataset to CSV and Parquet formats\n",
    "filepath = curr_wd + '/data/processed/'\n",
    "set3.to_csv(filepath + 'GAR_motif_Wang_set_human_cleaned.csv', index=False)\n",
    "set3.to_parquet(filepath + 'GAR_motif_Wang_set_human_cleaned.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# AGGREGATE ANNOTATIONS FOR THE ENTIRE HUMAN PROTEOME\n",
    "# This pre-fetches all data (domains, IDRs, PTMs, GO, LLPS) to avoid repeated lookups later.\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from feature_annotations import annot_domains, annot_PTMs, annot_IDR, annot_GO, annot_ELM\n",
    "\n",
    "# Paths\n",
    "annot_path = curr_wd + '/data/processed/annotation_datasets/'\n",
    "os.makedirs(annot_path + \"failed_requests\", exist_ok=True)\n",
    "\n",
    "# Load list of UniProt IDs\n",
    "protein_list = pd.read_csv(\n",
    "    curr_wd + '/data/processed/list_of_human_proteins.csv',\n",
    "    header=None,\n",
    "    names=[\"UniqueID\"]\n",
    ")['UniqueID'].tolist()\n",
    "\n",
    "##### DOMAINS #####\n",
    "failed_attempts, annotated_domains_df = annot_domains(\n",
    "    protein_list, return_failed_attempts=True, show_progress=True\n",
    ")\n",
    "annotated_domains_df.to_csv(annot_path + \"all_domains_human.csv\", index=False)\n",
    "annotated_domains_df.to_parquet(annot_path + \"all_domains_human.parquet\", index=False)\n",
    "failed_attempts.to_csv(annot_path + \"failed_requests/domains_human_failed_requests.csv\", index=False)\n",
    "\n",
    "# RUNTIME: ~1h\n",
    "\n",
    "##### IDRs #####\n",
    "failed_attempts, annotated_IDR_df = annot_IDR(\n",
    "    protein_list, return_failed_attempts=True, show_progress=True\n",
    ")\n",
    "annotated_IDR_df.to_csv(annot_path + \"all_IDR_human.csv\", index=False)\n",
    "annotated_IDR_df.to_parquet(annot_path + \"all_IDR_human.parquet\", index=False)\n",
    "failed_attempts.to_csv(annot_path + \"failed_requests/IDRs_human_failed_requests.csv\", index=False)\n",
    "\n",
    "# RUNTIME: ~75min\n",
    "\n",
    "##### PTMs #####\n",
    "failed_attempts, annotated_PTMs_df = annot_PTMs(\n",
    "    protein_list, return_failed_attempts=True, show_progress=True, verbose=True\n",
    ")\n",
    "annotated_PTMs_df.to_csv(annot_path + \"all_PTMs_human.csv\", index=False)\n",
    "annotated_PTMs_df.to_parquet(annot_path + \"all_PTMs_human.parquet\", index=False)\n",
    "failed_attempts.to_csv(annot_path + \"failed_requests/PTMs_human_failed_requests.csv\", index=False)\n",
    "\n",
    "# RUNTIME: ~86min\n",
    "\n",
    "##### GO Terms #####\n",
    "failed_attempts, annotated_GOs_df = annot_GO(\n",
    "    protein_list, return_failed_attempts=True, show_progress=True, verbose=True\n",
    ")\n",
    "annotated_GOs_df.to_csv(annot_path + \"all_GO_human.csv\", index=False)\n",
    "annotated_GOs_df.to_parquet(annot_path + \"all_GO_human.parquet\", index=False)\n",
    "failed_attempts.to_csv(annot_path + \"failed_requests/GO_human_failed_requests.csv\", index=False)\n",
    "\n",
    "# RUNTIME: ~39min\n",
    "\n",
    "##### LLPS Predictions (PhaSePred) #####\n",
    "df = pd.read_json(curr_wd + '/data/external/PhasePred/human_reviewed.json')\n",
    "df = df.T.reset_index().rename(columns={\"index\": \"UniqueID\"})\n",
    "llps_pred_df = df[['UniqueID', 'Sequence', 'PhaSePred']]\n",
    "llps_pred_df.to_csv(annot_path + \"phasepred_human.csv\", index=False)\n",
    "\n",
    "# RUNTIME: ~22 sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "annot_path = curr_wd + '/data/processed/annotation_datasets/'\n",
    "\n",
    "# Load annotation datasets\n",
    "annotated_domains_df = pd.read_csv(annot_path + \"all_domains_human.csv\")\n",
    "annotated_IDR_df = pd.read_parquet(annot_path + \"all_IDR_human.parquet\")\n",
    "annotated_PTMs_df = pd.read_csv(annot_path + \"all_PTMs_human.csv\")\n",
    "annotated_GO_df = pd.read_parquet(annot_path + \"all_GO_human.parquet\")\n",
    "\n",
    "llps_pred_df = pd.read_csv(annot_path + \"phasepred_human.csv\")\n",
    "llps_pred_df[\"PhaSePred\"] = llps_pred_df[\"PhaSePred\"].apply(literal_eval)\n",
    "\n",
    "dataset_base_name = \"GAR_motif_Wang_set_human\"\n",
    "motif_info_set_df = pd.read_parquet(curr_wd + '/data/processed/' + dataset_base_name + \"_cleaned.parquet\")\n",
    "\n",
    "##### GO TERMS #####\n",
    "go_terms_all, go_aspects_all = [], []\n",
    "invs_RNAbind_all, invs_DNAbind_all, invs_NAbind_all = [], [], []\n",
    "\n",
    "for _, motif in motif_info_set_df.iterrows():\n",
    "    curr_df = annotated_GO_df[annotated_GO_df['UniqueID'] == motif['UniqueID']]\n",
    "\n",
    "    if curr_df.empty:\n",
    "        go_terms_all.append([])\n",
    "        go_aspects_all.append([])\n",
    "        invs_RNAbind_all.append(0)\n",
    "        invs_DNAbind_all.append(0)\n",
    "        invs_NAbind_all.append(0)\n",
    "        continue\n",
    "\n",
    "    go_terms_all.append(curr_df[\"go_terms\"].iloc[0])\n",
    "    go_aspects_all.append(curr_df[\"go_aspects\"].iloc[0])\n",
    "    invs_RNAbind_all.append(curr_df[\"invs_RNAbind\"].iloc[0])\n",
    "    invs_DNAbind_all.append(curr_df[\"invs_DNAbind\"].iloc[0])\n",
    "    invs_NAbind_all.append(curr_df[\"invs_NAbind\"].iloc[0])\n",
    "\n",
    "motif_info_set_df['go_terms'] = go_terms_all\n",
    "motif_info_set_df['go_aspects'] = go_aspects_all\n",
    "motif_info_set_df['invs_RNAbind'] = invs_RNAbind_all\n",
    "motif_info_set_df['invs_DNAbind'] = invs_DNAbind_all\n",
    "motif_info_set_df['invs_NAbind'] = invs_NAbind_all\n",
    "\n",
    "##### DOMAINS #####\n",
    "part_of_domains, domain_names, domain_go, domain_distance = [], [], [], []\n",
    "\n",
    "for _, motif in motif_info_set_df.iterrows():\n",
    "    curr_df = annotated_domains_df[annotated_domains_df['protein_name'] == motif['UniqueID']]\n",
    "    part_temp, names_temp, go_temp, dist_temp = [], [], [], []\n",
    "\n",
    "    for _, domain in curr_df.iterrows():\n",
    "        names_temp.append(domain[\"name\"])\n",
    "        go_temp.append(domain[\"GO_identifiers\"])\n",
    "\n",
    "        if ((domain[\"start\"] >= motif[\"start\"] and domain[\"end\"] <= motif[\"end\"]) or\n",
    "            (motif[\"start\"] >= domain[\"start\"] and motif[\"end\"] <= domain[\"end\"])):\n",
    "            part_temp.append(True)\n",
    "            dist_temp.append(None)\n",
    "        else:\n",
    "            part_temp.append(False)\n",
    "            if domain[\"start\"] < motif[\"start\"]:\n",
    "                dist_temp.append(domain[\"end\"] - motif[\"start\"])\n",
    "            else:\n",
    "                dist_temp.append(domain[\"start\"] - motif[\"end\"])\n",
    "\n",
    "    part_of_domains.append(part_temp)\n",
    "    domain_names.append(names_temp)\n",
    "    domain_go.append(go_temp)\n",
    "    domain_distance.append(dist_temp)\n",
    "\n",
    "motif_info_set_df['part_of_domains'] = part_of_domains\n",
    "motif_info_set_df['domain_names'] = domain_names\n",
    "motif_info_set_df['domain_go'] = domain_go\n",
    "motif_info_set_df['domain_distance'] = domain_distance\n",
    "\n",
    "##### IDRs #####\n",
    "IDR_position_temp, IDR_overlap_temp = [], []\n",
    "\n",
    "for _, motif in motif_info_set_df.iterrows():\n",
    "    curr_df = annotated_IDR_df[annotated_IDR_df['protein_name'] == motif['UniqueID']]\n",
    "\n",
    "    if curr_df.empty:\n",
    "        # print(\"weirder fall, wo der eintrag nicht existiert\")\n",
    "        # print(motif[\"UniqueID\"])\n",
    "        IDR_overlap_temp.append(np.nan)\n",
    "        IDR_position_temp.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    seq = curr_df['prediction-disorder-mobidb_lite'].iloc[0]\n",
    "\n",
    "    if len(seq) <= motif['end']:\n",
    "        # print(\"weirder fall, wo die list zu lang oder zu kurz ist???\")\n",
    "        # print(motif[\"UniqueID\"])\n",
    "        IDR_overlap_temp.append(np.nan)\n",
    "        IDR_position_temp.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    tempslice = seq[motif['start']:motif['end']]\n",
    "    motif_middle = motif[\"start\"] + len(motif['motif']) // 2\n",
    "\n",
    "    if sum(tempslice) == len(tempslice):\n",
    "        IDR_overlap_temp.append(\"yes\")\n",
    "        count_right = next((j for j in range(1, len(seq) - motif_middle)\n",
    "                            if seq[motif_middle + j] != 1), len(seq) - motif_middle) - 1\n",
    "        count_left = next((k for k in range(1, motif_middle)\n",
    "                           if seq[motif_middle - k] != 1), motif_middle) - 1\n",
    "        IDR_position_temp.append((count_right - count_left) / (count_left + count_right))\n",
    "    elif sum(tempslice) > 0:\n",
    "        IDR_overlap_temp.append(\"partial\")\n",
    "        if tempslice[0] == 1:\n",
    "            IDR_position_temp.append(-1)\n",
    "        elif tempslice[-1] == 1:\n",
    "            IDR_position_temp.append(1)\n",
    "        else:\n",
    "            IDR_position_temp.append(0)\n",
    "    else:\n",
    "        IDR_overlap_temp.append(\"no\")\n",
    "        IDR_position_temp.append(None)\n",
    "\n",
    "print(\"IDRs done!\")\n",
    "motif_info_set_df['IDR_overlap'] = IDR_overlap_temp\n",
    "motif_info_set_df['IDR_position'] = IDR_position_temp\n",
    "\n",
    "##### PTMs #####\n",
    "PTM_names, PTM_distance, PTM_players, PTM_type = [], [], [], []\n",
    "\n",
    "for _, motif in motif_info_set_df.iterrows():\n",
    "    curr_df = annotated_PTMs_df[annotated_PTMs_df['protein_name'] == motif['UniqueID']]\n",
    "    names_temp, dist_temp, players_temp, type_temp = [], [], [], []\n",
    "\n",
    "    for _, ptm in curr_df.iterrows():\n",
    "        names_temp.append(ptm[\"ptm\"])\n",
    "        if ptm['pos'] < motif['start']:\n",
    "            dist_temp.append(ptm[\"pos\"] - motif['start'])\n",
    "        elif ptm[\"pos\"] < motif[\"end\"]:\n",
    "            dist_temp.append(0)\n",
    "        else:\n",
    "            dist_temp.append(ptm[\"pos\"] - motif['end'])\n",
    "        players_temp.append(ptm['by'])\n",
    "        type_temp.append(ptm['type'])\n",
    "\n",
    "    PTM_names.append(names_temp)\n",
    "    PTM_distance.append(dist_temp)\n",
    "    PTM_players.append(players_temp)\n",
    "    PTM_type.append(type_temp)\n",
    "\n",
    "motif_info_set_df['PTM_names'] = PTM_names\n",
    "motif_info_set_df['PTM_distance'] = PTM_distance\n",
    "motif_info_set_df['PTM_players'] = PTM_players\n",
    "motif_info_set_df['PTM_type'] = PTM_type\n",
    "\n",
    "##### LLPS #####\n",
    "llps_pred_score, llps_pred_rank = [], []\n",
    "\n",
    "for _, motif in motif_info_set_df.iterrows():\n",
    "    curr_df = llps_pred_df[llps_pred_df['UniqueID'] == motif['UniqueID']]\n",
    "    if not curr_df.empty:\n",
    "        llps_pred_score.append(curr_df[\"PhaSePred\"].iloc[0]['SaPS-8fea'])\n",
    "        llps_pred_rank.append(curr_df[\"PhaSePred\"].iloc[0]['PdPS-8fea'])\n",
    "    else:\n",
    "        llps_pred_score.append(np.nan)\n",
    "        llps_pred_rank.append(np.nan)\n",
    "\n",
    "motif_info_set_df['llps_pred_score'] = llps_pred_score\n",
    "motif_info_set_df['llps_pred_rank'] = llps_pred_rank\n",
    "\n",
    "# Save final annotated DataFrame\n",
    "motif_info_set_df.to_parquet(curr_wd + '/data/processed/' + dataset_base_name + \"_cleaned_annot.parquet\", index=False)\n",
    "motif_info_set_df.to_csv(curr_wd + '/data/processed/' + dataset_base_name + \"_cleaned_annot.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Final Venn diagram and group assignment for POS/NEG used in downstream analyses\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "os.makedirs(\"data/processed/final_set_lists/\", exist_ok=True)\n",
    "final_set_path = curr_wd + '/data/processed/final_set_lists/'\n",
    "\n",
    "def create_txt_files(true_subset_LLPS, true_subset_NA, GAR_subset, path_name):\n",
    "    LLPS_positive_set, NA_positive_set, GAR_subset = map(set, [true_subset_LLPS, true_subset_NA, GAR_subset])\n",
    "    set_collection = []\n",
    "    set_names_collection = []\n",
    "\n",
    "    set_collection.append(LLPS_positive_set - NA_positive_set - GAR_subset)\n",
    "    set_names_collection.append(\"1_LLPS_positive_set_only\")\n",
    "\n",
    "    set_collection.append(LLPS_positive_set & NA_positive_set - GAR_subset)\n",
    "    set_names_collection.append(\"2_LLPS_positive_set_and_NA_positive_set\")\n",
    "\n",
    "    set_collection.append(NA_positive_set - LLPS_positive_set - GAR_subset)\n",
    "    set_names_collection.append(\"3_NA_positive_set_only\")\n",
    "\n",
    "    set_collection.append(LLPS_positive_set & GAR_subset - NA_positive_set)\n",
    "    set_names_collection.append(\"4_LLPS_positive_set_and_GAR_subset\")\n",
    "\n",
    "    set_collection.append(LLPS_positive_set & NA_positive_set & GAR_subset)\n",
    "    set_names_collection.append(\"5_LLPS_positive_set_and_NA_positive_set_and_GAR_subset\")\n",
    "\n",
    "    set_collection.append(NA_positive_set & GAR_subset - LLPS_positive_set)\n",
    "    set_names_collection.append(\"6_NA_positive_set_and_GAR_subset\")\n",
    "\n",
    "    set_collection.append(GAR_subset - LLPS_positive_set - NA_positive_set)\n",
    "    set_names_collection.append(\"7_GAR_subset_only\")\n",
    "\n",
    "    set_collection.extend([true_subset_LLPS, true_subset_NA, GAR_subset])\n",
    "    set_names_collection.extend([\"LLPS_positive_set_full\", \"NA_positive_set_full\", \"GAR_subset_full\"])\n",
    "\n",
    "    for i, sets in enumerate(set_collection):\n",
    "        with open(path_name + str(set_names_collection[i]) + \".txt\", 'w') as fp:\n",
    "            for item in sets:\n",
    "                fp.write(f\"{item}\\n\")\n",
    "\n",
    "# Load and prepare data\n",
    "motif_info_set_df = pd.read_parquet(curr_wd + '/data/processed/' + dataset_base_name + \"_cleaned_annot.parquet\")\n",
    "\n",
    "llps_pred_df = pd.read_csv(annot_path + \"phasepred_human.csv\")\n",
    "\n",
    "llps_pred_df[\"PhaSePred\"] = llps_pred_df[\"PhaSePred\"].apply(literal_eval)\n",
    "llps_pred_df[\"SaPS-8fea\"] = [a[\"SaPS-8fea\"] for a in llps_pred_df[\"PhaSePred\"]]\n",
    "llps_pred_df[\"PdPS-8fea\"] = [a[\"PdPS-8fea\"] for a in llps_pred_df[\"PhaSePred\"]]\n",
    "\n",
    "annotated_GO_df = pd.read_parquet(annot_path + \"all_GO_human.parquet\")\n",
    "\n",
    "true_subset_LLPS = llps_pred_df[llps_pred_df[\"SaPS-8fea\"] > 0.5][\"UniqueID\"].tolist()\n",
    "true_subset_RNA = annotated_GO_df[annotated_GO_df[\"invs_RNAbind\"] > 0][\"UniqueID\"].tolist()\n",
    "true_subset_DNA = annotated_GO_df[annotated_GO_df[\"invs_DNAbind\"] > 0][\"UniqueID\"].tolist()\n",
    "true_subset_NA = annotated_GO_df[annotated_GO_df[\"invs_NAbind\"] > 0][\"UniqueID\"].tolist()\n",
    "true_subset = [value for value in true_subset_LLPS if value in true_subset_RNA]\n",
    "\n",
    "# === Version 2 Filtering: IDR-only, not part of any domain ===\n",
    "filtered_df = motif_info_set_df[motif_info_set_df['part_of_domains'].apply(lambda x: all(v == False for v in x))]\n",
    "\n",
    "filtered_df = filtered_df[filtered_df[\"IDR_overlap\"] == \"yes\"]\n",
    "# === Version 3: Remove Collagen-associated proteins ===\n",
    "collagen_file = curr_wd + '/data/external/UniProt/collagen_prots_human.txt'\n",
    "coll_list_prot = pd.read_csv(collagen_file, sep='\\t').Entry.tolist()\n",
    "\n",
    "print(\"Removal of Collagen associated proteins\")\n",
    "print(\"Before:\", len(filtered_df[\"UniqueID\"].tolist()))\n",
    "filtered_df = filtered_df[~filtered_df[\"UniqueID\"].isin(coll_list_prot)]\n",
    "print(\"After:\", len(filtered_df[\"UniqueID\"].tolist()))\n",
    "\n",
    "# Save V3 data\n",
    "temp_filtered_df = filtered_df.reset_index(drop=False)\n",
    "temp_filtered_df.rename(columns={\"index\": \"orig_motif_index\"}, inplace=True)\n",
    "temp_filtered_df.to_parquet(curr_wd + '/data/processed/' + dataset_base_name + \"_cleaned_annot_filtered.parquet\")\n",
    "\n",
    "# Generate and save Venn diagram\n",
    "GAR_subset = filtered_df[\"UniqueID\"].tolist()\n",
    "create_txt_files(true_subset_LLPS, true_subset_NA, GAR_subset, final_set_path)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "venn = venn3([set(true_subset_LLPS), set(true_subset_NA), set(GAR_subset)], set_labels=('I', 'II', 'III'), set_colors=('#96DB70', '#DB8663', '#636DDB'), alpha=0.8)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(7, 7))\n",
    "# venn3([set(true_subset_LLPS), set(true_subset_NA), set(GAR_subset)],\n",
    "#       (\"A\", 'B', 'C'), set_colors=['blue', 'red', 'green'])\n",
    "# venn3_circles([set(true_subset_LLPS), set(true_subset_NA), set(GAR_subset)], linestyle='dotted')\n",
    "# plt.gcf().savefig(final_set_path + 'GARvsLLPSvsNA_Venn.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib_venn import venn3\n",
    "\n",
    "# # Define the subset sizes\n",
    "# subsets = (1900, 2275, 1073, 230, 182, 58, 193) ##### these values come from the lsits above\n",
    "# # Create the figure\n",
    "\n",
    "# # Create the figure\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# venn = venn3(subsets=subsets, set_labels=('I', 'II', 'III'), set_colors=('#96DB70', '#DB8663', '#636DDB'), alpha=0.8)\n",
    "\n",
    "# Customize appearance for publication\n",
    "for subset in venn.set_labels:\n",
    "    subset.set_fontsize(18)\n",
    "    # subset.set_fontweight('bold')\n",
    "\n",
    "for i in [3]:\n",
    "    x, y = venn.subset_labels[i].get_position()\n",
    "    venn.subset_labels[i].set_position((x, y + 0.04))  # Adjust x_offset and y_offset as needed\n",
    "    # venn.subset_labels[i].set_color(\"#FF4040\")\n",
    "\n",
    "for i in [4]:\n",
    "    x, y = venn.subset_labels[i].get_position()\n",
    "    venn.subset_labels[i].set_position((x+0.02, y - 0.01))  # Adjust x_offset and y_offset as needed\n",
    "\n",
    "for i in [5]:\n",
    "    x, y = venn.subset_labels[i].get_position()\n",
    "    venn.subset_labels[i].set_position((x-0.01, y - 0.01))  # Adjust x_offset and y_offset as needed\n",
    "    # venn.subset_labels[i].set_color(\"red\")\n",
    "for i in [6]:\n",
    "    x, y = venn.subset_labels[i].get_position()\n",
    "    venn.subset_labels[i].set_position((x-0.01, y - 0.03))  # Adjust x_offset and y_offset as needed\n",
    "    # venn.subset_labels[i].set_color(\"#8DB600\")\n",
    "\n",
    "for subset in venn.subset_labels:\n",
    "    if subset:\n",
    "        subset.set_fontsize(12)\n",
    "\n",
    "# Highlight specific regions\n",
    "# venn.get_label_by_id('111').set_bbox(dict(facecolor='none', edgecolor='green', linewidth=3))  # Center intersection\n",
    "# venn.get_label_by_id('001').set_bbox(dict(facecolor='none', edgecolor='red', linewidth=3))  # Bottom intersection\n",
    "\n",
    "# Add dashed outline to circles\n",
    "for i in [0,1,2,3,6,4,5]:\n",
    "    patch = venn.patches[i]\n",
    "    if patch:\n",
    "        patch.set_edgecolor('grey')\n",
    "        patch.set_linestyle('solid')\n",
    "        patch.set_linewidth(2)\n",
    "# venn.patches[3].set_edgecolor(\"#FF4040\")\n",
    "# venn.patches[3].set_linewidth(3)\n",
    "# venn.patches[6].set_edgecolor(\"#8DB600\")\n",
    "# venn.patches[6].set_linewidth(3)\n",
    "\n",
    "\n",
    "# Set title\n",
    "# plt.title(\"Venn Diagram of Three Sets\", fontsize=16, fontweight='bold')\n",
    "os.makedirs(\"data/results/subfigures/\", exist_ok=True)\n",
    "plt.savefig(curr_wd + '/data/results/subfigures/' +  \"fig1_A.svg\", transparent=True)\n",
    "# plt.savefig(final_set_path + 'GARvsLLPSvsNA_Venn.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
